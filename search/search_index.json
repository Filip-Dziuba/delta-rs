{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Python deltalake package","text":"<p>This is the documentation for the native Python implementation of Delta Lake. It is based on the delta-rs Rust library and requires no Spark or JVM dependencies. For the PySpark implementation, see delta-spark instead.</p> <p>This module provides the capability to read, write, and manage Delta Lake tables from Python without Spark or Java. It uses Apache Arrow under the hood, so is compatible with other Arrow-native or integrated libraries such as Pandas, DuckDB, and Polars.</p> <p>Note: This module is under active development and some features are experimental. It is not yet as feature-complete as the PySpark implementation of Delta Lake. If you encounter a bug, please let us know in our GitHub repo.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#using-pip","title":"Using Pip","text":"<pre><code>pip install deltalake\n</code></pre> <p>NOTE: official binary wheels are linked against openssl statically for remote objection store communication. Please file Github issue to request for critical openssl upgrade.</p>"},{"location":"api/delta_table/","title":"DeltaTable","text":""},{"location":"api/delta_table/#deltalake.table.DeltaTable","title":"DeltaTable  <code>dataclass</code>","text":"<pre><code>DeltaTable(table_uri: Union[str, Path], version: Optional[int] = None, storage_options: Optional[Dict[str, str]] = None, without_files: bool = False, log_buffer_size: Optional[int] = None)\n</code></pre> <p>Represents a Delta Table</p> <p>Create the Delta Table from a path with an optional version. Multiple StorageBackends are currently supported: AWS S3, Azure Data Lake Storage Gen2, Google Cloud Storage (GCS) and local URI. Depending on the storage backend used, you could provide options values using the <code>storage_options</code> parameter.</p> <p>Parameters:</p> Name Type Description Default <code>table_uri</code> <code>Union[str, Path]</code> <p>the path of the DeltaTable</p> required <code>version</code> <code>Optional[int]</code> <p>version of the DeltaTable</p> <code>None</code> <code>storage_options</code> <code>Optional[Dict[str, str]]</code> <p>a dictionary of the options to use for the storage backend</p> <code>None</code> <code>without_files</code> <code>bool</code> <p>If True, will load table without tracking files.                 Some append-only applications might have no need of tracking any files. So, the                 DeltaTable will be loaded with a significant memory reduction.</p> <code>False</code> <code>log_buffer_size</code> <code>Optional[int]</code> <p>Number of files to buffer when reading the commit log. A positive integer.                 Setting a value greater than 1 results in concurrent calls to the storage api.                 This can decrease latency if there are many files in the log since the last checkpoint,                 but will also increase memory usage. Possible rate limits of the storage backend should                 also be considered for optimal performance. Defaults to 4 * number of cpus.</p> <code>None</code>"},{"location":"api/delta_table/#deltalake.table.DeltaTable.cleanup_metadata","title":"cleanup_metadata","text":"<pre><code>cleanup_metadata() -&gt; None\n</code></pre> <p>Delete expired log files before current version from table. The table log retention is based on the <code>configuration.logRetentionDuration</code> value, 30 days by default.</p>"},{"location":"api/delta_table/#deltalake.table.DeltaTable.delete","title":"delete","text":"<pre><code>delete(predicate: Optional[str] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Delete records from a Delta Table that statisfy a predicate.</p> <p>When a predicate is not provided then all records are deleted from the Delta Table. Otherwise a scan of the Delta table is performed to mark any files that contain records that satisfy the predicate. Once files are determined they are rewritten without the records.</p> <p>Parameters:</p> Name Type Description Default <code>predicate</code> <code>Optional[str]</code> <p>a SQL where clause. If not passed, will delete all rows.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>the metrics from delete.</p>"},{"location":"api/delta_table/#deltalake.table.DeltaTable.file_uris","title":"file_uris","text":"<pre><code>file_uris(partition_filters: Optional[List[Tuple[str, str, Any]]] = None) -&gt; List[str]\n</code></pre> <p>Get the list of files as absolute URIs, including the scheme (e.g. \"s3://\").</p> <p>Local files will be just plain absolute paths, without a scheme. (That is, no 'file://' prefix.)</p> <p>Use the partition_filters parameter to retrieve a subset of files that match the given filters.</p> <p>Parameters:</p> Name Type Description Default <code>partition_filters</code> <code>Optional[List[Tuple[str, str, Any]]]</code> <p>the partition filters that will be used for getting the matched files</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>list of the .parquet files with an absolute URI referenced for the current version of the DeltaTable</p> <p>Predicates are expressed in disjunctive normal form (DNF), like [(\"x\", \"=\", \"a\"), ...]. DNF allows arbitrary boolean logical combinations of single partition predicates. The innermost tuples each describe a single partition predicate. The list of inner predicates is interpreted as a conjunction (AND), forming a more selective and multiple partition predicates. Each tuple has format: (key, op, value) and compares the key with the value. The supported op are: <code>=</code>, <code>!=</code>, <code>in</code>, and <code>not in</code>. If the op is in or not in, the value must be a collection such as a list, a set or a tuple. The supported type for value is str. Use empty string <code>''</code> for Null partition value.</p> <p>Examples: <pre><code>(\"x\", \"=\", \"a\")\n(\"x\", \"!=\", \"a\")\n(\"y\", \"in\", [\"a\", \"b\", \"c\"])\n(\"z\", \"not in\", [\"a\",\"b\"])\n</code></pre></p>"},{"location":"api/delta_table/#deltalake.table.DeltaTable.files","title":"files","text":"<pre><code>files(partition_filters: Optional[List[Tuple[str, str, Any]]] = None) -&gt; List[str]\n</code></pre> <p>Get the .parquet files of the DeltaTable.</p> <p>The paths are as they are saved in the delta log, which may either be relative to the table root or absolute URIs.</p> <p>Parameters:</p> Name Type Description Default <code>partition_filters</code> <code>Optional[List[Tuple[str, str, Any]]]</code> <p>the partition filters that will be used for                 getting the matched files</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>list of the .parquet files referenced for the current version of the DeltaTable</p> <p>Predicates are expressed in disjunctive normal form (DNF), like [(\"x\", \"=\", \"a\"), ...]. DNF allows arbitrary boolean logical combinations of single partition predicates. The innermost tuples each describe a single partition predicate. The list of inner predicates is interpreted as a conjunction (AND), forming a more selective and multiple partition predicates. Each tuple has format: (key, op, value) and compares the key with the value. The supported op are: <code>=</code>, <code>!=</code>, <code>in</code>, and <code>not in</code>. If the op is in or not in, the value must be a collection such as a list, a set or a tuple. The supported type for value is str. Use empty string <code>''</code> for Null partition value.</p> <p>Examples: <pre><code>(\"x\", \"=\", \"a\")\n(\"x\", \"!=\", \"a\")\n(\"y\", \"in\", [\"a\", \"b\", \"c\"])\n(\"z\", \"not in\", [\"a\",\"b\"])\n</code></pre></p>"},{"location":"api/delta_table/#deltalake.table.DeltaTable.from_data_catalog","title":"from_data_catalog  <code>classmethod</code>","text":"<pre><code>from_data_catalog(data_catalog: DataCatalog, database_name: str, table_name: str, data_catalog_id: Optional[str] = None, version: Optional[int] = None, log_buffer_size: Optional[int] = None) -&gt; DeltaTable\n</code></pre> <p>Create the Delta Table from a Data Catalog.</p> <p>Parameters:</p> Name Type Description Default <code>data_catalog</code> <code>DataCatalog</code> <p>the Catalog to use for getting the storage location of the Delta Table</p> required <code>database_name</code> <code>str</code> <p>the database name inside the Data Catalog</p> required <code>table_name</code> <code>str</code> <p>the table name inside the Data Catalog</p> required <code>data_catalog_id</code> <code>Optional[str]</code> <p>the identifier of the Data Catalog</p> <code>None</code> <code>version</code> <code>Optional[int]</code> <p>version of the DeltaTable</p> <code>None</code> <code>log_buffer_size</code> <code>Optional[int]</code> <p>Number of files to buffer when reading the commit log. A positive integer.                 Setting a value greater than 1 results in concurrent calls to the storage api.                 This can decrease latency if there are many files in the log since the last checkpoint,                 but will also increase memory usage. Possible rate limits of the storage backend should                 also be considered for optimal performance. Defaults to 4 * number of cpus.</p> <code>None</code>"},{"location":"api/delta_table/#deltalake.table.DeltaTable.get_add_actions","title":"get_add_actions","text":"<pre><code>get_add_actions(flatten: bool = False) -&gt; pyarrow.RecordBatch\n</code></pre> <p>Return a dataframe with all current add actions.</p> <p>Add actions represent the files that currently make up the table. This data is a low-level representation parsed from the transaction log.</p> <p>Parameters:</p> Name Type Description Default <code>flatten</code> <code>bool</code> <p>whether to flatten the schema. Partition values columns are         given the prefix <code>partition.</code>, statistics (null_count, min, and max) are         given the prefix <code>null_count.</code>, <code>min.</code>, and <code>max.</code>, and tags the         prefix <code>tags.</code>. Nested field names are concatenated with <code>.</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>RecordBatch</code> <p>a PyArrow RecordBatch containing the add action data.</p> <p>Example: <pre><code>from deltalake import DeltaTable, write_deltalake\nimport pyarrow as pa\ndata = pa.table({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\nwrite_deltalake(\"tmp\", data, partition_by=[\"x\"])\ndt = DeltaTable(\"tmp\")\ndt.get_add_actions().to_pandas()\n</code></pre> <pre><code>path                                                size_bytes       modification_time  data_change partition_values  num_records null_count       min       max\n0  x=2/0-91820cbf-f698-45fb-886d-5d5f5669530b-0.p...         565 1970-01-20 08:40:08.071         True         {'x': 2}            1   {'y': 0}  {'y': 5}  {'y': 5}\n1  x=3/0-91820cbf-f698-45fb-886d-5d5f5669530b-0.p...         565 1970-01-20 08:40:08.071         True         {'x': 3}            1   {'y': 0}  {'y': 6}  {'y': 6}\n2  x=1/0-91820cbf-f698-45fb-886d-5d5f5669530b-0.p...         565 1970-01-20 08:40:08.071         True         {'x': 1}            1   {'y': 0}  {'y': 4}  {'y': 4}\n</code></pre></p> <p><pre><code>dt.get_add_actions(flatten=True).to_pandas()\n</code></pre> <pre><code>path                                                size_bytes       modification_time  data_change  partition.x  num_records  null_count.y  min.y  max.y\n0  x=2/0-91820cbf-f698-45fb-886d-5d5f5669530b-0.p...         565 1970-01-20 08:40:08.071         True            2            1             0      5      5\n1  x=3/0-91820cbf-f698-45fb-886d-5d5f5669530b-0.p...         565 1970-01-20 08:40:08.071         True            3            1             0      6      6\n2  x=1/0-91820cbf-f698-45fb-886d-5d5f5669530b-0.p...         565 1970-01-20 08:40:08.071         True            1            1             0      4      4\n</code></pre></p>"},{"location":"api/delta_table/#deltalake.table.DeltaTable.history","title":"history","text":"<pre><code>history(limit: Optional[int] = None) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Run the history command on the DeltaTable. The operations are returned in reverse chronological order.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>Optional[int]</code> <p>the commit info limit to return</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>list of the commit infos registered in the transaction log</p>"},{"location":"api/delta_table/#deltalake.table.DeltaTable.load_version","title":"load_version","text":"<pre><code>load_version(version: int) -&gt; None\n</code></pre> <p>Load a DeltaTable with a specified version.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>int</code> <p>the identifier of the version of the DeltaTable to load</p> required"},{"location":"api/delta_table/#deltalake.table.DeltaTable.load_with_datetime","title":"load_with_datetime","text":"<pre><code>load_with_datetime(datetime_string: str) -&gt; None\n</code></pre> <p>Time travel Delta table to the latest version that's created at or before provided <code>datetime_string</code> argument. The <code>datetime_string</code> argument should be an RFC 3339 and ISO 8601 date and time string.</p> <p>Parameters:</p> Name Type Description Default <code>datetime_string</code> <code>str</code> <p>the identifier of the datetime point of the DeltaTable to load</p> required <p>Examples: <pre><code>\"2018-01-26T18:30:09Z\"\n\"2018-12-19T16:39:57-08:00\"\n\"2018-01-26T18:30:09.453+00:00\"\n</code></pre></p>"},{"location":"api/delta_table/#deltalake.table.DeltaTable.merge","title":"merge","text":"<pre><code>merge(source: Union[pyarrow.Table, pyarrow.RecordBatch, pyarrow.RecordBatchReader], predicate: str, source_alias: Optional[str] = None, target_alias: Optional[str] = None, error_on_type_mismatch: bool = True) -&gt; TableMerger\n</code></pre> <p>Pass the source data which you want to merge on the target delta table, providing a predicate in SQL query like format. You can also specify on what to do when the underlying data types do not match the underlying table.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Table | RecordBatch | RecordBatchReader</code> <p>source data</p> required <code>predicate</code> <code>str</code> <p>SQL like predicate on how to merge</p> required <code>source_alias</code> <code>str</code> <p>Alias for the source table</p> <code>None</code> <code>target_alias</code> <code>str</code> <p>Alias for the target table</p> <code>None</code> <code>error_on_type_mismatch</code> <code>bool</code> <p>specify if merge will return error if data types are mismatching :default = True</p> <code>True</code> <p>Returns:</p> Name Type Description <code>TableMerger</code> <code>TableMerger</code> <p>TableMerger Object</p>"},{"location":"api/delta_table/#deltalake.table.DeltaTable.metadata","title":"metadata","text":"<pre><code>metadata() -&gt; Metadata\n</code></pre> <p>Get the current metadata of the DeltaTable.</p> <p>Returns:</p> Type Description <code>Metadata</code> <p>the current Metadata registered in the transaction log</p>"},{"location":"api/delta_table/#deltalake.table.DeltaTable.protocol","title":"protocol","text":"<pre><code>protocol() -&gt; ProtocolVersions\n</code></pre> <p>Get the reader and writer protocol versions of the DeltaTable.</p> <p>Returns:</p> Type Description <code>ProtocolVersions</code> <p>the current ProtocolVersions registered in the transaction log</p>"},{"location":"api/delta_table/#deltalake.table.DeltaTable.repair","title":"repair","text":"<pre><code>repair(dry_run: bool = False) -&gt; Dict[str, Any]\n</code></pre> <p>Repair the Delta Table by auditing active files that do not exist in the underlying filesystem and removes them. This can be useful when there are accidental deletions or corrupted files.</p> <p>Active files are ones that have an add action in the log, but no corresponding remove action. This operation creates a new FSCK transaction containing a remove action for each of the missing or corrupted files.</p> <p>Parameters:</p> Name Type Description Default <code>dry_run</code> <code>bool</code> <p>when activated, list only the files, otherwise add remove actions to transaction log. Defaults to False.</p> <code>False</code> <p>Returns:     The metrics from repair (FSCK) action.</p> <p>Examples: <pre><code>from deltalake import DeltaTable\ndt = DeltaTable('TEST')\ndt.repair(dry_run=False)\n</code></pre> Results in <pre><code>{'dry_run': False, 'files_removed': ['6-0d084325-6885-4847-b008-82c1cf30674c-0.parquet', 5-4fba1d3e-3e20-4de1-933d-a8e13ac59f53-0.parquet']}\n</code></pre></p>"},{"location":"api/delta_table/#deltalake.table.DeltaTable.restore","title":"restore","text":"<pre><code>restore(target: Union[int, datetime, str], *, ignore_missing_files: bool = False, protocol_downgrade_allowed: bool = False) -&gt; Dict[str, Any]\n</code></pre> <p>Run the Restore command on the Delta Table: restore table to a given version or datetime.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Union[int, datetime, str]</code> <p>the expected version will restore, which represented by int, date str or datetime.</p> required <code>ignore_missing_files</code> <code>bool</code> <p>whether the operation carry on when some data files missing.</p> <code>False</code> <code>protocol_downgrade_allowed</code> <code>bool</code> <p>whether the operation when protocol version upgraded.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>the metrics from restore.</p>"},{"location":"api/delta_table/#deltalake.table.DeltaTable.schema","title":"schema","text":"<pre><code>schema() -&gt; Schema\n</code></pre> <p>Get the current schema of the DeltaTable.</p> <p>Returns:</p> Type Description <code>Schema</code> <p>the current Schema registered in the transaction log</p>"},{"location":"api/delta_table/#deltalake.table.DeltaTable.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas(partitions: Optional[List[Tuple[str, str, Any]]] = None, columns: Optional[List[str]] = None, filesystem: Optional[Union[str, pa_fs.FileSystem]] = None, filters: Optional[FilterType] = None) -&gt; pandas.DataFrame\n</code></pre> <p>Build a pandas dataframe using data from the DeltaTable.</p> <p>Parameters:</p> Name Type Description Default <code>partitions</code> <code>Optional[List[Tuple[str, str, Any]]]</code> <p>A list of partition filters, see help(DeltaTable.files_by_partitions) for filter syntax</p> <code>None</code> <code>columns</code> <code>Optional[List[str]]</code> <p>The columns to project. This can be a list of column names to include (order and duplicates will be preserved)</p> <code>None</code> <code>filesystem</code> <code>Optional[Union[str, FileSystem]]</code> <p>A concrete implementation of the Pyarrow FileSystem or a fsspec-compatible interface. If None, the first file path will be used to determine the right FileSystem</p> <code>None</code> <code>filters</code> <code>Optional[FilterType]</code> <p>A disjunctive normal form (DNF) predicate for filtering rows. If you pass a filter you do not need to pass <code>partitions</code></p> <code>None</code>"},{"location":"api/delta_table/#deltalake.table.DeltaTable.to_pyarrow_dataset","title":"to_pyarrow_dataset","text":"<pre><code>to_pyarrow_dataset(partitions: Optional[List[Tuple[str, str, Any]]] = None, filesystem: Optional[Union[str, pa_fs.FileSystem]] = None, parquet_read_options: Optional[ParquetReadOptions] = None) -&gt; pyarrow.dataset.Dataset\n</code></pre> <p>Build a PyArrow Dataset using data from the DeltaTable.</p> <p>Parameters:</p> Name Type Description Default <code>partitions</code> <code>Optional[List[Tuple[str, str, Any]]]</code> <p>A list of partition filters, see help(DeltaTable.files_by_partitions) for filter syntax</p> <code>None</code> <code>filesystem</code> <code>Optional[Union[str, FileSystem]]</code> <p>A concrete implementation of the Pyarrow FileSystem or a fsspec-compatible interface. If None, the first file path will be used to determine the right FileSystem</p> <code>None</code> <code>parquet_read_options</code> <code>Optional[ParquetReadOptions]</code> <p>Optional read options for Parquet. Use this to handle INT96 to timestamp conversion for edge cases like 0001-01-01 or 9999-12-31</p> <code>None</code> <p>More info: https://arrow.apache.org/docs/python/generated/pyarrow.dataset.ParquetReadOptions.html</p> <p>Returns:</p> Type Description <code>Dataset</code> <p>the PyArrow dataset in PyArrow</p>"},{"location":"api/delta_table/#deltalake.table.DeltaTable.to_pyarrow_table","title":"to_pyarrow_table","text":"<pre><code>to_pyarrow_table(partitions: Optional[List[Tuple[str, str, Any]]] = None, columns: Optional[List[str]] = None, filesystem: Optional[Union[str, pa_fs.FileSystem]] = None, filters: Optional[FilterType] = None) -&gt; pyarrow.Table\n</code></pre> <p>Build a PyArrow Table using data from the DeltaTable.</p> <p>Parameters:</p> Name Type Description Default <code>partitions</code> <code>Optional[List[Tuple[str, str, Any]]]</code> <p>A list of partition filters, see help(DeltaTable.files_by_partitions) for filter syntax</p> <code>None</code> <code>columns</code> <code>Optional[List[str]]</code> <p>The columns to project. This can be a list of column names to include (order and duplicates will be preserved)</p> <code>None</code> <code>filesystem</code> <code>Optional[Union[str, FileSystem]]</code> <p>A concrete implementation of the Pyarrow FileSystem or a fsspec-compatible interface. If None, the first file path will be used to determine the right FileSystem</p> <code>None</code> <code>filters</code> <code>Optional[FilterType]</code> <p>A disjunctive normal form (DNF) predicate for filtering rows. If you pass a filter you do not need to pass <code>partitions</code></p> <code>None</code>"},{"location":"api/delta_table/#deltalake.table.DeltaTable.update","title":"update","text":"<pre><code>update(updates: Optional[Dict[str, str]] = None, new_values: Optional[Dict[str, Union[int, float, str, datetime, bool, List[Any]]]] = None, predicate: Optional[str] = None, writer_properties: Optional[Dict[str, int]] = None, error_on_type_mismatch: bool = True) -&gt; Dict[str, Any]\n</code></pre> <p><code>UPDATE</code> records in the Delta Table that matches an optional predicate. Either updates or new_values needs to be passed for it to execute.</p> <p>Parameters:</p> Name Type Description Default <code>updates</code> <code>Optional[Dict[str, str]]</code> <p>a mapping of column name to update SQL expression.</p> <code>None</code> <code>new_values</code> <code>Optional[Dict[str, Union[int, float, str, datetime, bool, List[Any]]]]</code> <p>a mapping of column name to python datatype.</p> <code>None</code> <code>predicate</code> <code>Optional[str]</code> <p>a logical expression, defaults to None</p> <code>None</code> <code>writer_properties</code> <code>Optional[Dict[str, int]]</code> <p>Pass writer properties to the Rust parquet writer, see options https://arrow.apache.org/rust/parquet/file/properties/struct.WriterProperties.html, only the following fields are supported: <code>data_page_size_limit</code>, <code>dictionary_page_size_limit</code>, <code>data_page_row_count_limit</code>, <code>write_batch_size</code>, <code>max_row_group_size</code>.</p> <code>None</code> <code>error_on_type_mismatch</code> <code>bool</code> <p>specify if update will return error if data types are mismatching :default = True</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>the metrics from update</p> <p>Examples:</p> <p>Update some row values with SQL predicate. This is equivalent to <code>UPDATE table SET deleted = true WHERE id = '5'</code></p> <pre><code>from deltalake import DeltaTable\ndt = DeltaTable(\"tmp\")\ndt.update(predicate=\"id = '5'\", updates = {\"deleted\": 'True'})\n</code></pre> <p>Update all row values. This is equivalent to <code>UPDATE table SET deleted = true, id = concat(id, '_old')</code>. <pre><code>from deltalake import DeltaTable\ndt = DeltaTable(\"tmp\")\ndt.update(updates = {\"deleted\": 'True', \"id\": \"concat(id, '_old')\"})\n</code></pre></p> <p>To use Python objects instead of SQL strings, use the <code>new_values</code> parameter instead of the <code>updates</code> parameter. For example, this is equivalent to <code>UPDATE table SET price = 150.10 WHERE id = '5'</code> <pre><code>from deltalake import DeltaTable\ndt = DeltaTable(\"tmp\")\ndt.update(predicate=\"id = '5'\", new_values = {\"price\": 150.10})\n</code></pre></p>"},{"location":"api/delta_table/#deltalake.table.DeltaTable.update_incremental","title":"update_incremental","text":"<pre><code>update_incremental() -&gt; None\n</code></pre> <p>Updates the DeltaTable to the latest version by incrementally applying newer versions.</p>"},{"location":"api/delta_table/#deltalake.table.DeltaTable.vacuum","title":"vacuum","text":"<pre><code>vacuum(retention_hours: Optional[int] = None, dry_run: bool = True, enforce_retention_duration: bool = True) -&gt; List[str]\n</code></pre> <p>Run the Vacuum command on the Delta Table: list and delete files no longer referenced by the Delta table and are older than the retention threshold.</p> <p>Parameters:</p> Name Type Description Default <code>retention_hours</code> <code>Optional[int]</code> <p>the retention threshold in hours, if none then the value from <code>configuration.deletedFileRetentionDuration</code> is used or default of 1 week otherwise.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>when activated, list only the files, delete otherwise</p> <code>True</code> <code>enforce_retention_duration</code> <code>bool</code> <p>when disabled, accepts retention hours smaller than the value from <code>configuration.deletedFileRetentionDuration</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>the list of files no longer referenced by the Delta Table and are older than the retention threshold.</p>"},{"location":"api/delta_table/#deltalake.table.DeltaTable.version","title":"version","text":"<pre><code>version() -&gt; int\n</code></pre> <p>Get the version of the DeltaTable.</p> <p>Returns:</p> Type Description <code>int</code> <p>The current version of the DeltaTable</p>"},{"location":"api/delta_table/#deltalake.table.Metadata","title":"Metadata  <code>dataclass</code>","text":"<pre><code>Metadata(table: RawDeltaTable)\n</code></pre> <p>Create a Metadata instance.</p>"},{"location":"api/delta_table/#deltalake.table.Metadata.configuration","title":"configuration  <code>property</code>","text":"<pre><code>configuration: Dict[str, str]\n</code></pre> <p>Return the DeltaTable properties.</p>"},{"location":"api/delta_table/#deltalake.table.Metadata.created_time","title":"created_time  <code>property</code>","text":"<pre><code>created_time: int\n</code></pre> <p>Return The time when this metadata action is created, in milliseconds since the Unix epoch of the DeltaTable.</p>"},{"location":"api/delta_table/#deltalake.table.Metadata.description","title":"description  <code>property</code>","text":"<pre><code>description: str\n</code></pre> <p>Return the user-provided description of the DeltaTable.</p>"},{"location":"api/delta_table/#deltalake.table.Metadata.id","title":"id  <code>property</code>","text":"<pre><code>id: int\n</code></pre> <p>Return the unique identifier of the DeltaTable.</p>"},{"location":"api/delta_table/#deltalake.table.Metadata.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Return the user-provided identifier of the DeltaTable.</p>"},{"location":"api/delta_table/#deltalake.table.Metadata.partition_columns","title":"partition_columns  <code>property</code>","text":"<pre><code>partition_columns: List[str]\n</code></pre> <p>Return an array containing the names of the partitioned columns of the DeltaTable.</p>"},{"location":"api/delta_table/#deltalake.table.TableMerger","title":"TableMerger","text":"<pre><code>TableMerger(table: DeltaTable, source: pyarrow.RecordBatchReader, predicate: str, source_alias: Optional[str] = None, target_alias: Optional[str] = None, safe_cast: bool = True)\n</code></pre> <p>API for various table <code>MERGE</code> commands.</p>"},{"location":"api/delta_table/#deltalake.table.TableMerger.execute","title":"execute","text":"<pre><code>execute() -&gt; Dict[str, Any]\n</code></pre> <p>Executes <code>MERGE</code> with the previously provided settings in Rust with Apache Datafusion query engine.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: metrics</p>"},{"location":"api/delta_table/#deltalake.table.TableMerger.when_matched_delete","title":"when_matched_delete","text":"<pre><code>when_matched_delete(predicate: Optional[str] = None) -&gt; TableMerger\n</code></pre> <p>Delete a matched row from the table only if the given <code>predicate</code> (if specified) is true for the matched row. If not specified it deletes all matches.</p> <p>Parameters:</p> Name Type Description Default <code>predicate</code> <code>(str | None, Optional)</code> <p>SQL like predicate on when to delete. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>TableMerger</code> <code>TableMerger</code> <p>TableMerger Object</p> <p>Examples:</p> <p>Delete on a predicate <pre><code>from deltalake import DeltaTable\nimport pyarrow as pa\ndata = pa.table({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\ndt = DeltaTable(\"tmp\")\n(             dt.merge(                 source=data,                 predicate='target.x = source.x',                 source_alias='source',                 target_alias='target')             .when_matched_delete(                 predicate = \"source.deleted = true\")             .execute()         ```\nDelete all records that were matched\n</code></pre> from deltalake import DeltaTable import pyarrow as pa data = pa.table({\"x\": [1, 2, 3], \"y\": [4, 5, 6]}) dt = DeltaTable(\"tmp\") (             dt.merge(                 source=data,                 predicate='target.x = source.x',                 source_alias='source',                 target_alias='target')               .when_matched_delete()              .execute()         ```</p>"},{"location":"api/delta_table/#deltalake.table.TableMerger.when_matched_update","title":"when_matched_update","text":"<pre><code>when_matched_update(updates: Dict[str, str], predicate: Optional[str] = None) -&gt; TableMerger\n</code></pre> <p>Update a matched table row based on the rules defined by <code>updates</code>. If a <code>predicate</code> is specified, then it must evaluate to true for the row to be updated.</p> <p>Parameters:</p> Name Type Description Default <code>updates</code> <code>dict</code> <p>a mapping of column name to update SQL expression.</p> required <code>predicate</code> <code>(str | None, Optional)</code> <p>SQL like predicate on when to update. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>TableMerger</code> <code>TableMerger</code> <p>TableMerger Object</p> <p>Examples: <pre><code>from deltalake import DeltaTable\nimport pyarrow as pa\ndata = pa.table({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\ndt = DeltaTable(\"tmp\")\n(             dt.merge(                 source=data,                 predicate=\"target.x = source.x\",                 source_alias=\"source\",                 target_alias=\"target\")             .when_matched_update(updates={\"x\": \"source.x\", \"y\": \"source.y\"})             .execute()         )\n</code></pre></p>"},{"location":"api/delta_table/#deltalake.table.TableMerger.when_matched_update_all","title":"when_matched_update_all","text":"<pre><code>when_matched_update_all(predicate: Optional[str] = None) -&gt; TableMerger\n</code></pre> <p>Updating all source fields to target fields, source and target are required to have the same field names.  If a <code>predicate</code> is specified, then it must evaluate to true for the row to be updated.</p> <p>Parameters:</p> Name Type Description Default <code>predicate</code> <code>(str | None, Optional)</code> <p>SQL like predicate on when to update all columns. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>TableMerger</code> <code>TableMerger</code> <p>TableMerger Object</p> <p>Examples:</p> <pre><code>from deltalake import DeltaTable\nimport pyarrow as pa\ndata = pa.table({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\ndt = DeltaTable(\"tmp\")\n(            dt.merge(                 source=data,                 predicate='target.x = source.x',                 source_alias='source',                 target_alias='target')              .when_matched_update_all()             .execute()             )\n</code></pre>"},{"location":"api/delta_table/#deltalake.table.TableMerger.when_not_matched_by_source_delete","title":"when_not_matched_by_source_delete","text":"<pre><code>when_not_matched_by_source_delete(predicate: Optional[str] = None) -&gt; TableMerger\n</code></pre> <p>Delete a target row that has no matches in the source from the table only if the given <code>predicate</code> (if specified) is true for the target row.</p> <p>Parameters:</p> Name Type Description Default <code>predicate</code> <code>(str | None, Optional)</code> <p>SQL like predicate on when to delete when not matched by source. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>TableMerger</code> <code>TableMerger</code> <p>TableMerger Object</p>"},{"location":"api/delta_table/#deltalake.table.TableMerger.when_not_matched_by_source_update","title":"when_not_matched_by_source_update","text":"<pre><code>when_not_matched_by_source_update(updates: Dict[str, str], predicate: Optional[str] = None) -&gt; TableMerger\n</code></pre> <p>Update a target row that has no matches in the source based on the rules defined by <code>updates</code>. If a <code>predicate</code> is specified, then it must evaluate to true for the row to be updated.</p> <p>Parameters:</p> Name Type Description Default <code>updates</code> <code>dict</code> <p>a mapping of column name to update SQL expression.</p> required <code>predicate</code> <code>(str | None, Optional)</code> <p>SQL like predicate on when to update. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>TableMerger</code> <code>TableMerger</code> <p>TableMerger Object</p> <p><code>from deltalake import DeltaTable import pyarrow as pa data = pa.table({\"x\": [1, 2, 3], \"y\": [4, 5, 6]}) dt = DeltaTable(\"tmp\") (             dt.merge(                 source=data,                 predicate='target.x = source.x',                 source_alias='source',                 target_alias='target')             .when_not_matched_by_source_update(                 predicate = \"y &gt; 3\",                 updates = {\"y\": \"0\"})             .execute()             )</code></p>"},{"location":"api/delta_table/#deltalake.table.TableMerger.when_not_matched_insert","title":"when_not_matched_insert","text":"<pre><code>when_not_matched_insert(updates: Dict[str, str], predicate: Optional[str] = None) -&gt; TableMerger\n</code></pre> <p>Insert a new row to the target table based on the rules defined by <code>updates</code>. If a <code>predicate</code> is specified, then it must evaluate to true for the new row to be inserted.</p> <p>Parameters:</p> Name Type Description Default <code>updates</code> <code>dict</code> <p>a mapping of column name to insert SQL expression.</p> required <code>predicate</code> <code>(str | None, Optional)</code> <p>SQL like predicate on when to insert. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>TableMerger</code> <code>TableMerger</code> <p>TableMerger Object</p> <p>Examples:</p> <pre><code>from deltalake import DeltaTable\nimport pyarrow as pa\ndata = pa.table({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\ndt = DeltaTable(\"tmp\")\n(             dt.merge(                 source=data,                 predicate='target.x = source.x',                 source_alias='source',                 target_alias='target')             .when_not_matched_insert(                  updates = {                      \"x\": \"source.x\",                      \"y\": \"source.y\",                          })             .execute()                 )\n</code></pre>"},{"location":"api/delta_table/#deltalake.table.TableMerger.when_not_matched_insert_all","title":"when_not_matched_insert_all","text":"<pre><code>when_not_matched_insert_all(predicate: Optional[str] = None) -&gt; TableMerger\n</code></pre> <p>Insert a new row to the target table, updating all source fields to target fields. Source and target are  required to have the same field names. If a <code>predicate</code> is specified, then it must evaluate to true for  the new row to be inserted.</p> <p>Parameters:</p> Name Type Description Default <code>predicate</code> <code>(str | None, Optional)</code> <p>SQL like predicate on when to insert. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>TableMerger</code> <code>TableMerger</code> <p>TableMerger Object</p> <p>Examples:</p> <pre><code>from deltalake import DeltaTable\nimport pyarrow as pa\ndata = pa.table({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\ndt = DeltaTable(\"tmp\")\n(             dt             .merge(                 source=data,                 predicate='target.x = source.x',                 source_alias='source',                 target_alias='target')             .when_not_matched_insert_all()             .execute()              )\n</code></pre>"},{"location":"api/delta_table/#deltalake.table.TableMerger.with_writer_properties","title":"with_writer_properties","text":"<pre><code>with_writer_properties(data_page_size_limit: Optional[int] = None, dictionary_page_size_limit: Optional[int] = None, data_page_row_count_limit: Optional[int] = None, write_batch_size: Optional[int] = None, max_row_group_size: Optional[int] = None) -&gt; TableMerger\n</code></pre> <p>Pass writer properties to the Rust parquet writer, see options https://arrow.apache.org/rust/parquet/file/properties/struct.WriterProperties.html:</p> <p>Parameters:</p> Name Type Description Default <code>data_page_size_limit</code> <code>(int | None, Optional)</code> <p>Limit DataPage size to this in bytes. Defaults to None.</p> <code>None</code> <code>dictionary_page_size_limit</code> <code>(int | None, Optional)</code> <p>Limit the size of each DataPage to store dicts to this amount in bytes. Defaults to None.</p> <code>None</code> <code>data_page_row_count_limit</code> <code>(int | None, Optional)</code> <p>Limit the number of rows in each DataPage. Defaults to None.</p> <code>None</code> <code>write_batch_size</code> <code>(int | None, Optional)</code> <p>Splits internally to smaller batch size. Defaults to None.</p> <code>None</code> <code>max_row_group_size</code> <code>(int | None, Optional)</code> <p>Max number of rows in row group. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>TableMerger</code> <code>TableMerger</code> <p>TableMerger Object</p>"},{"location":"api/delta_table/#deltalake.table.TableOptimizer","title":"TableOptimizer","text":"<pre><code>TableOptimizer(table: DeltaTable)\n</code></pre> <p>API for various table optimization commands.</p>"},{"location":"api/delta_table/#deltalake.table.TableOptimizer.compact","title":"compact","text":"<pre><code>compact(partition_filters: Optional[FilterType] = None, target_size: Optional[int] = None, max_concurrent_tasks: Optional[int] = None, min_commit_interval: Optional[Union[int, timedelta]] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Compacts small files to reduce the total number of files in the table.</p> <p>This operation is idempotent; if run twice on the same table (assuming it has not been updated) it will do nothing the second time.</p> <p>If this operation happens concurrently with any operations other than append, it will fail.</p> <p>Parameters:</p> Name Type Description Default <code>partition_filters</code> <code>Optional[FilterType]</code> <p>the partition filters that will be used for getting the matched files</p> <code>None</code> <code>target_size</code> <code>Optional[int]</code> <p>desired file size after bin-packing files, in bytes. If not             provided, will attempt to read the table configuration value <code>delta.targetFileSize</code>.             If that value isn't set, will use default value of 256MB.</p> <code>None</code> <code>max_concurrent_tasks</code> <code>Optional[int]</code> <p>the maximum number of concurrent tasks to use for                     file compaction. Defaults to number of CPUs. More concurrent tasks can make compaction                     faster, but will also use more memory.</p> <code>None</code> <code>min_commit_interval</code> <code>Optional[Union[int, timedelta]]</code> <p>minimum interval in seconds or as timedeltas before a new commit is                     created. Interval is useful for long running executions. Set to 0 or timedelta(0), if you                     want a commit per partition.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>the metrics from optimize</p> <p>Example: <pre><code># Use a timedelta object to specify the seconds, minutes or hours of the interval.\nfrom deltalake import DeltaTable\nfrom datetime import timedelta\ndt = DeltaTable(\"tmp\")\ntime_delta = timedelta(minutes=10)\ndt.optimize.z_order([\"timestamp\"], min_commit_interval=time_delta)\n</code></pre></p>"},{"location":"api/delta_table/#deltalake.table.TableOptimizer.z_order","title":"z_order","text":"<pre><code>z_order(columns: Iterable[str], partition_filters: Optional[FilterType] = None, target_size: Optional[int] = None, max_concurrent_tasks: Optional[int] = None, max_spill_size: int = 20 * 1024 * 1024 * 1024, min_commit_interval: Optional[Union[int, timedelta]] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Reorders the data using a Z-order curve to improve data skipping.</p> <p>This also performs compaction, so the same parameters as compact() apply.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>Iterable[str]</code> <p>the columns to use for Z-ordering. There must be at least one column.         partition_filters: the partition filters that will be used for getting the matched files</p> required <code>target_size</code> <code>Optional[int]</code> <p>desired file size after bin-packing files, in bytes. If not             provided, will attempt to read the table configuration value <code>delta.targetFileSize</code>.             If that value isn't set, will use default value of 256MB.</p> <code>None</code> <code>max_concurrent_tasks</code> <code>Optional[int]</code> <p>the maximum number of concurrent tasks to use for                     file compaction. Defaults to number of CPUs. More concurrent tasks can make compaction                     faster, but will also use more memory.</p> <code>None</code> <code>max_spill_size</code> <code>int</code> <p>the maximum number of bytes to spill to disk. Defaults to 20GB.</p> <code>20 * 1024 * 1024 * 1024</code> <code>min_commit_interval</code> <code>Optional[Union[int, timedelta]]</code> <p>minimum interval in seconds or as timedeltas before a new commit is                     created. Interval is useful for long running executions. Set to 0 or timedelta(0), if you                     want a commit per partition.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>the metrics from optimize</p> <p>Example: <pre><code># Use a timedelta object to specify the seconds, minutes or hours of the interval.\nfrom deltalake import DeltaTable\nfrom datetime import timedelta\ndt = DeltaTable(\"tmp\")\ntime_delta = timedelta(minutes=10)\ndt.optimize.compact(min_commit_interval=time_delta)\n</code></pre></p>"},{"location":"api/delta_table/#writing-delta-tables","title":"Writing Delta Tables","text":"<p>Write to a Delta Lake table</p> <p>If the table does not already exist, it will be created.</p> <p>This function only supports writer protocol version 2 currently. When attempting to write to an existing table with a higher min_writer_version, this function will throw DeltaProtocolError.</p> <p>Note that this function does NOT register this table in a data catalog.</p> <p>A locking mechanism is needed to prevent unsafe concurrent writes to a delta lake directory when writing to S3. DynamoDB is the only available locking provider at the moment in delta-rs. To enable DynamoDB as the locking provider, you need to set the <code>AWS_S3_LOCKING_PROVIDER</code> to 'dynamodb' as a storage_option or as an environment variable.</p> <p>Additionally, you must create a DynamoDB table with the name 'delta_rs_lock_table' so that it can be automatically discovered by delta-rs. Alternatively, you can use a table name of your choice, but you must set the <code>DYNAMO_LOCK_TABLE_NAME</code> variable to match your chosen table name. The required schema for the DynamoDB table is as follows:</p> <ul> <li>Key Schema: AttributeName=key, KeyType=HASH</li> <li>Attribute Definitions: AttributeName=key, AttributeType=S</li> </ul> <p>Please note that this locking mechanism is not compatible with any other locking mechanisms, including the one used by Spark.</p> <p>Parameters:</p> Name Type Description Default <code>table_or_uri</code> <code>Union[str, Path, DeltaTable]</code> <p>URI of a table or a DeltaTable object.</p> required <code>data</code> <code>Union[DataFrame, Dataset, Table, RecordBatch, Iterable[RecordBatch], RecordBatchReader]</code> <p>Data to write. If passing iterable, the schema must also be given.</p> required <code>schema</code> <code>Optional[Schema]</code> <p>Optional schema to write.</p> <code>None</code> <code>partition_by</code> <code>Optional[Union[List[str], str]]</code> <p>List of columns to partition the table by. Only required when creating a new table.</p> <code>None</code> <code>filesystem</code> <code>Optional[FileSystem]</code> <p>Optional filesystem to pass to PyArrow. If not provided will be inferred from uri. The file system has to be rooted in the table root. Use the pyarrow.fs.SubTreeFileSystem, to adopt the root of pyarrow file systems.</p> <code>None</code> <code>mode</code> <code>Literal['error', 'append', 'overwrite', 'ignore']</code> <p>How to handle existing data. Default is to error if table already exists. If 'append', will add new data. If 'overwrite', will replace table with new data. If 'ignore', will not write anything if table already exists.</p> <code>'error'</code> <code>file_options</code> <code>Optional[ParquetFileWriteOptions]</code> <p>Optional write options for Parquet (ParquetFileWriteOptions). Can be provided with defaults using ParquetFileWriteOptions().make_write_options(). Please refer to https://github.com/apache/arrow/blob/master/python/pyarrow/_dataset_parquet.pyx#L492-L533 for the list of available options</p> <code>None</code> <code>max_partitions</code> <code>Optional[int]</code> <p>the maximum number of partitions that will be used.</p> <code>None</code> <code>max_open_files</code> <code>int</code> <p>Limits the maximum number of files that can be left open while writing. If an attempt is made to open too many files then the least recently used file will be closed. If this setting is set too low you may end up fragmenting your data into many small files.</p> <code>1024</code> <code>max_rows_per_file</code> <code>int</code> <p>Maximum number of rows per file. If greater than 0 then this will limit how many rows are placed in any single file. Otherwise there will be no limit and one file will be created in each output directory unless files need to be closed to respect max_open_files min_rows_per_group: Minimum number of rows per group. When the value is set, the dataset writer will batch incoming data and only write the row groups to the disk when sufficient rows have accumulated.</p> <code>10 * 1024 * 1024</code> <code>max_rows_per_group</code> <code>int</code> <p>Maximum number of rows per group. If the value is set, then the dataset writer may split up large incoming batches into multiple row groups. If this value is set, then min_rows_per_group should also be set.</p> <code>128 * 1024</code> <code>name</code> <code>Optional[str]</code> <p>User-provided identifier for this table.</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>User-provided description for this table.</p> <code>None</code> <code>configuration</code> <code>Optional[Mapping[str, Optional[str]]]</code> <p>A map containing configuration options for the metadata action.</p> <code>None</code> <code>overwrite_schema</code> <code>bool</code> <p>If True, allows updating the schema of the table.</p> <code>False</code> <code>storage_options</code> <code>Optional[Dict[str, str]]</code> <p>options passed to the native delta filesystem. Unused if 'filesystem' is defined.</p> <code>None</code> <code>partition_filters</code> <code>Optional[List[Tuple[str, str, Any]]]</code> <p>the partition filters that will be used for partition overwrite.</p> <code>None</code> <code>large_dtypes</code> <code>bool</code> <p>If True, the table schema is checked against large_dtypes</p> <code>False</code>"},{"location":"api/schema/","title":"Schema","text":""},{"location":"api/schema/#delta-lake-schemas","title":"Delta Lake Schemas","text":"<p>Schemas, fields, and data types are provided in the <code>deltalake.schema</code> submodule.</p>"},{"location":"api/schema/#deltalake.schema.Schema","title":"deltalake.schema.Schema","text":"<pre><code>Schema(fields: List[Field])\n</code></pre> <p>             Bases: <code>deltalake._internal.StructType</code></p> <p>A Delta Lake schema</p> <p>Create using a list of :class:<code>Field</code>:</p> <p>Schema([Field(\"x\", \"integer\"), Field(\"y\", \"string\")]) Schema([Field(x, PrimitiveType(\"integer\"), nullable=True), Field(y, PrimitiveType(\"string\"), nullable=True)])</p> <p>Or create from a PyArrow schema:</p> <p>import pyarrow as pa Schema.from_pyarrow(pa.schema({\"x\": pa.int32(), \"y\": pa.string()})) Schema([Field(x, PrimitiveType(\"integer\"), nullable=True), Field(y, PrimitiveType(\"string\"), nullable=True)])</p>"},{"location":"api/schema/#deltalake.schema.Schema.invariants","title":"invariants","text":"<pre><code>invariants: List[Tuple[str, str]] = &lt;attribute 'invariants' of 'deltalake._internal.Schema' objects&gt;\n</code></pre>"},{"location":"api/schema/#deltalake.schema.Schema.from_json","title":"from_json  <code>staticmethod</code>","text":"<pre><code>from_json(schema_json) -&gt; Schema\n</code></pre> <p>Create a new Schema from a JSON string.</p> <p>A schema has the same JSON format as a StructType. <pre><code>Schema.from_json('''{\n    \"type\": \"struct\",\n    \"fields\": [{\"name\": \"x\", \"type\": \"integer\", \"nullable\": true, \"metadata\": {}}]\n    }\n)'''\n# Returns Schema([Field(x, PrimitiveType(\"integer\"), nullable=True)])\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>json</code> <code>str</code> <p>a JSON string</p> required"},{"location":"api/schema/#deltalake.schema.Schema.from_pyarrow","title":"from_pyarrow  <code>staticmethod</code>","text":"<pre><code>from_pyarrow(data_type) -&gt; Schema\n</code></pre> <p>Create a Schema from a PyArrow Schema type</p> <p>Will raise <code>TypeError</code> if the PyArrow type is not a primitive type.</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <code>Schema</code> <p>A PyArrow Schema type</p> required <p>Returns: a Schema type</p>"},{"location":"api/schema/#deltalake.schema.Schema.to_json","title":"to_json  <code>method descriptor</code>","text":"<pre><code>to_json() -&gt; str\n</code></pre> <p>Get the JSON string representation of the Schema. A schema has the same JSON format as a StructType. <pre><code>Schema([Field(\"x\", \"integer\")]).to_json()\n# Returns '{\"type\":\"struct\",\"fields\":[{\"name\":\"x\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}}]}'\n</code></pre> Returns: a JSON string</p>"},{"location":"api/schema/#deltalake.schema.Schema.to_pyarrow","title":"to_pyarrow  <code>method descriptor</code>","text":"<pre><code>to_pyarrow(as_large_types: bool = False) -&gt; pyarrow.Schema\n</code></pre> <p>Return equivalent PyArrow schema</p> <p>Parameters:</p> Name Type Description Default <code>as_large_types</code> <code>bool</code> <p>get schema with all variable size types (list, binary, string) as large variants (with int64 indices). This is for compatibility with systems like Polars that only support the large versions of Arrow types.</p> <code>False</code> <p>Returns:</p> Type Description <code>Schema</code> <p>a PyArrow Schema type</p>"},{"location":"api/schema/#deltalake.schema.PrimitiveType","title":"deltalake.schema.PrimitiveType","text":"<pre><code>PrimitiveType(data_type: str)\n</code></pre>"},{"location":"api/schema/#deltalake.schema.PrimitiveType.type","title":"type","text":"<pre><code>type: str = &lt;attribute 'type' of 'deltalake._internal.PrimitiveType' objects&gt;\n</code></pre>"},{"location":"api/schema/#deltalake.schema.PrimitiveType.from_json","title":"from_json  <code>staticmethod</code>","text":"<pre><code>from_json(type_json) -&gt; PrimitiveType\n</code></pre> <p>Create a PrimitiveType from a JSON string</p> <p>The JSON representation for a primitive type is just a quoted string: <code>PrimitiveType.from_json('\"integer\"')</code></p> <p>Parameters:</p> Name Type Description Default <code>json</code> <code>str</code> <p>A JSON string</p> required <p>Returns a PrimitiveType type</p>"},{"location":"api/schema/#deltalake.schema.PrimitiveType.from_pyarrow","title":"from_pyarrow  <code>staticmethod</code>","text":"<pre><code>from_pyarrow(data_type) -&gt; PrimitiveType\n</code></pre> <p>Create a PrimitiveType from a PyArrow type</p> <p>Will raise <code>TypeError</code> if the PyArrow type is not a primitive type.</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <code>DataType</code> <p>A PyArrow DataType type</p> required <p>Returns: a PrimitiveType type</p>"},{"location":"api/schema/#deltalake.schema.PrimitiveType.to_pyarrow","title":"to_pyarrow  <code>method descriptor</code>","text":"<pre><code>to_pyarrow() -&gt; pyarrow.DataType\n</code></pre> <p>Get the equivalent PyArrow type (pyarrow.DataType)</p>"},{"location":"api/schema/#deltalake.schema.ArrayType","title":"deltalake.schema.ArrayType","text":"<pre><code>ArrayType(element_type: DataType, *, contains_null: bool = True)\n</code></pre>"},{"location":"api/schema/#deltalake.schema.ArrayType.contains_null","title":"contains_null","text":"<pre><code>contains_null: bool = &lt;attribute 'contains_null' of 'deltalake._internal.ArrayType' objects&gt;\n</code></pre>"},{"location":"api/schema/#deltalake.schema.ArrayType.element_type","title":"element_type","text":"<pre><code>element_type: DataType = &lt;attribute 'element_type' of 'deltalake._internal.ArrayType' objects&gt;\n</code></pre>"},{"location":"api/schema/#deltalake.schema.ArrayType.type","title":"type","text":"<pre><code>type: Literal['array'] = &lt;attribute 'type' of 'deltalake._internal.ArrayType' objects&gt;\n</code></pre>"},{"location":"api/schema/#deltalake.schema.ArrayType.from_json","title":"from_json  <code>staticmethod</code>","text":"<pre><code>from_json(type_json) -&gt; ArrayType\n</code></pre> <p>Create an ArrayType from a JSON string</p> <p>The JSON representation for an array type is an object with <code>type</code> (set to <code>\"array\"</code>), <code>elementType</code>, and <code>containsNull</code>: <pre><code>ArrayType.from_json(\n    '''{\n        \"type\": \"array\",\n        \"elementType\": \"integer\",\n        \"containsNull\": false\n    }'''\n)\n# Returns ArrayType(PrimitiveType(\"integer\"), contains_null=False)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>json</code> <code>str</code> <p>A JSON string</p> required <p>Returns: an ArrayType type</p>"},{"location":"api/schema/#deltalake.schema.ArrayType.from_pyarrow","title":"from_pyarrow  <code>staticmethod</code>","text":"<pre><code>from_pyarrow(data_type) -&gt; ArrayType\n</code></pre> <p>Create an ArrayType from a pyarrow.ListType.</p> <p>Will raise <code>TypeError</code> if a different PyArrow DataType is provided.</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <code>ListType</code> <p>The PyArrow ListType</p> required <p>Returns: an ArrayType type</p>"},{"location":"api/schema/#deltalake.schema.ArrayType.to_json","title":"to_json  <code>method descriptor</code>","text":"<pre><code>to_json() -&gt; str\n</code></pre> <p>Get the JSON string representation of the type.</p>"},{"location":"api/schema/#deltalake.schema.ArrayType.to_pyarrow","title":"to_pyarrow  <code>method descriptor</code>","text":"<pre><code>to_pyarrow() -&gt; pyarrow.ListType\n</code></pre> <p>Get the equivalent PyArrow type.</p>"},{"location":"api/schema/#deltalake.schema.MapType","title":"deltalake.schema.MapType","text":"<pre><code>MapType(key_type: DataType, value_type: DataType, *, value_contains_null: bool = True)\n</code></pre>"},{"location":"api/schema/#deltalake.schema.MapType.key_type","title":"key_type","text":"<pre><code>key_type: DataType = &lt;attribute 'key_type' of 'deltalake._internal.MapType' objects&gt;\n</code></pre>"},{"location":"api/schema/#deltalake.schema.MapType.type","title":"type","text":"<pre><code>type: Literal['map'] = &lt;attribute 'type' of 'deltalake._internal.MapType' objects&gt;\n</code></pre>"},{"location":"api/schema/#deltalake.schema.MapType.value_contains_null","title":"value_contains_null","text":"<pre><code>value_contains_null: bool = &lt;attribute 'value_contains_null' of 'deltalake._internal.MapType' objects&gt;\n</code></pre>"},{"location":"api/schema/#deltalake.schema.MapType.value_type","title":"value_type","text":"<pre><code>value_type: DataType = &lt;attribute 'value_type' of 'deltalake._internal.MapType' objects&gt;\n</code></pre>"},{"location":"api/schema/#deltalake.schema.MapType.from_json","title":"from_json  <code>staticmethod</code>","text":"<pre><code>from_json(type_json) -&gt; MapType\n</code></pre> <p>Create a MapType from a JSON string</p> <p>The JSON representation for a map type is an object with <code>type</code> (set to <code>map</code>), <code>keyType</code>, <code>valueType</code>, and <code>valueContainsNull</code>: <pre><code>MapType.from_json(\n    '''{\n        \"type\": \"map\",\n        \"keyType\": \"integer\",\n        \"valueType\": \"string\",\n        \"valueContainsNull\": true\n    }'''\n)\n# Returns MapType(PrimitiveType(\"integer\"), PrimitiveType(\"string\"), value_contains_null=True)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>json</code> <code>str</code> <p>A JSON string</p> required <p>Returns: a MapType type</p>"},{"location":"api/schema/#deltalake.schema.MapType.from_pyarrow","title":"from_pyarrow  <code>staticmethod</code>","text":"<pre><code>from_pyarrow(data_type) -&gt; MapType\n</code></pre> <p>Create a MapType from a PyArrow MapType.</p> <p>Will raise <code>TypeError</code> if passed a different type.</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <code>MapType</code> <p>the PyArrow MapType</p> required <p>Returns: a MapType type</p>"},{"location":"api/schema/#deltalake.schema.MapType.to_json","title":"to_json  <code>method descriptor</code>","text":"<pre><code>to_json() -&gt; str\n</code></pre> <p>Get JSON string representation of map type.</p>"},{"location":"api/schema/#deltalake.schema.MapType.to_pyarrow","title":"to_pyarrow  <code>method descriptor</code>","text":"<pre><code>to_pyarrow() -&gt; pyarrow.MapType\n</code></pre> <p>Get the equivalent PyArrow data type.</p>"},{"location":"api/schema/#deltalake.schema.Field","title":"deltalake.schema.Field","text":"<pre><code>Field(name: str, type: DataType, *, nullable: bool = True, metadata: Optional[Dict[str, Any]] = None)\n</code></pre>"},{"location":"api/schema/#deltalake.schema.Field.metadata","title":"metadata","text":"<pre><code>metadata: Dict[str, Any] = &lt;attribute 'metadata' of 'deltalake._internal.Field' objects&gt;\n</code></pre>"},{"location":"api/schema/#deltalake.schema.Field.name","title":"name","text":"<pre><code>name: str = &lt;attribute 'name' of 'deltalake._internal.Field' objects&gt;\n</code></pre>"},{"location":"api/schema/#deltalake.schema.Field.nullable","title":"nullable","text":"<pre><code>nullable: bool = &lt;attribute 'nullable' of 'deltalake._internal.Field' objects&gt;\n</code></pre>"},{"location":"api/schema/#deltalake.schema.Field.type","title":"type","text":"<pre><code>type: DataType = &lt;attribute 'type' of 'deltalake._internal.Field' objects&gt;\n</code></pre>"},{"location":"api/schema/#deltalake.schema.Field.from_json","title":"from_json  <code>staticmethod</code>","text":"<pre><code>from_json(field_json) -&gt; Field\n</code></pre> <p>Create a Field from a JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>json</code> <code>str</code> <p>the JSON string.</p> required <p>Returns: Field</p> <p>Example: <pre><code>Field.from_json('''{\n        \"name\": \"col\",\n        \"type\": \"integer\",\n        \"nullable\": true,\n        \"metadata\": {}\n    }'''\n)\n# Returns Field(col, PrimitiveType(\"integer\"), nullable=True)\n</code></pre></p>"},{"location":"api/schema/#deltalake.schema.Field.from_pyarrow","title":"from_pyarrow  <code>staticmethod</code>","text":"<pre><code>from_pyarrow(field: pyarrow.Field) -&gt; Field\n</code></pre> <p>Create a Field from a PyArrow field Note: This currently doesn't preserve field metadata.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>Field</code> <p>a PyArrow Field type</p> required <p>Returns: a Field type</p>"},{"location":"api/schema/#deltalake.schema.Field.to_json","title":"to_json  <code>method descriptor</code>","text":"<pre><code>to_json() -&gt; str\n</code></pre> <p>Get the field as JSON string. <pre><code>Field(\"col\", \"integer\").to_json()\n# Returns '{\"name\":\"col\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}}'\n</code></pre></p>"},{"location":"api/schema/#deltalake.schema.Field.to_pyarrow","title":"to_pyarrow  <code>method descriptor</code>","text":"<pre><code>to_pyarrow() -&gt; pyarrow.Field\n</code></pre> <p>Convert to an equivalent PyArrow field Note: This currently doesn't preserve field metadata.</p> <p>Returns: a pyarrow.Field type</p>"},{"location":"api/schema/#deltalake.schema.StructType","title":"deltalake.schema.StructType","text":"<pre><code>StructType(fields: List[Field])\n</code></pre>"},{"location":"api/schema/#deltalake.schema.StructType.fields","title":"fields","text":"<pre><code>fields: List[Field] = &lt;attribute 'fields' of 'deltalake._internal.StructType' objects&gt;\n</code></pre>"},{"location":"api/schema/#deltalake.schema.StructType.type","title":"type","text":"<pre><code>type: Literal['struct'] = &lt;attribute 'type' of 'deltalake._internal.StructType' objects&gt;\n</code></pre> <p>The string \"struct\"</p>"},{"location":"api/schema/#deltalake.schema.StructType.from_json","title":"from_json  <code>staticmethod</code>","text":"<pre><code>from_json(type_json) -&gt; StructType\n</code></pre> <p>Create a new StructType from a JSON string. <pre><code>StructType.from_json(\n    '''{\n        \"type\": \"struct\",\n        \"fields\": [{\"name\": \"x\", \"type\": \"integer\", \"nullable\": true, \"metadata\": {}}]\n    }'''\n)\n# Returns StructType([Field(x, PrimitiveType(\"integer\"), nullable=True)])\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>json</code> <code>str</code> <p>a JSON string</p> required <p>Returns: a StructType type</p>"},{"location":"api/schema/#deltalake.schema.StructType.from_pyarrow","title":"from_pyarrow  <code>staticmethod</code>","text":"<pre><code>from_pyarrow(data_type) -&gt; StructType\n</code></pre> <p>Create a new StructType from a PyArrow struct type.</p> <p>Will raise <code>TypeError</code> if a different data type is provided.</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <code>StructType</code> <p>a PyArrow struct type.</p> required <p>Returns: a StructType type</p>"},{"location":"api/schema/#deltalake.schema.StructType.to_json","title":"to_json  <code>method descriptor</code>","text":"<pre><code>to_json() -&gt; str\n</code></pre> <p>Get the JSON representation of the type. <pre><code>StructType([Field(\"x\", \"integer\")]).to_json()\n# Returns '{\"type\":\"struct\",\"fields\":[{\"name\":\"x\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}}]}'\n</code></pre></p>"},{"location":"api/schema/#deltalake.schema.StructType.to_pyarrow","title":"to_pyarrow  <code>method descriptor</code>","text":"<pre><code>to_pyarrow() -&gt; pyarrow.StructType\n</code></pre> <p>Get the equivalent PyArrow StructType</p> <p>Returns: a PyArrow StructType type</p>"},{"location":"api/schema/#deltalake.data_catalog.DataCatalog","title":"DataCatalog","text":"<p>             Bases: <code>Enum</code></p> <p>List of the Data Catalogs</p>"},{"location":"api/schema/#deltalake.data_catalog.DataCatalog.AWS","title":"AWS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>AWS = 'glue'\n</code></pre> <p>Refers to the <code>AWS Glue Data Catalog &lt;https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html&gt;</code>_</p>"},{"location":"api/schema/#deltalake.data_catalog.DataCatalog.UNITY","title":"UNITY  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>UNITY = 'unity'\n</code></pre> <p>Refers to the <code>Databricks Unity Catalog &lt;https://docs.databricks.com/data-governance/unity-catalog/index.html&gt;</code>_</p>"},{"location":"api/schema/#delta-storage-handler","title":"Delta Storage Handler","text":""},{"location":"api/schema/#deltalake.fs.DeltaStorageHandler","title":"DeltaStorageHandler","text":"<pre><code>DeltaStorageHandler(root: str, options: dict[str, str] | None = None, known_sizes: dict[str, int] | None = None)\n</code></pre> <p>             Bases: <code>DeltaFileSystemHandler</code>, <code>FileSystemHandler</code></p> <p>DeltaStorageHandler is a concrete implementations of a PyArrow FileSystemHandler.</p>"},{"location":"api/schema/#deltalake.fs.DeltaStorageHandler.get_file_info_selector","title":"get_file_info_selector","text":"<pre><code>get_file_info_selector(selector: FileSelector) -&gt; List[FileInfo]\n</code></pre> <p>Get info for the files defined by FileSelector.</p> <p>Parameters:</p> Name Type Description Default <code>selector</code> <code>FileSelector</code> <p>FileSelector object</p> required <p>Returns:</p> Type Description <code>List[FileInfo]</code> <p>list of file info objects</p>"},{"location":"api/schema/#deltalake.fs.DeltaStorageHandler.open_input_file","title":"open_input_file","text":"<pre><code>open_input_file(path: str) -&gt; pa.PythonFile\n</code></pre> <p>Open an input file for random access reading.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The source to open for reading.</p> required <p>Returns:</p> Type Description <code>PythonFile</code> <p>NativeFile</p>"},{"location":"api/schema/#deltalake.fs.DeltaStorageHandler.open_input_stream","title":"open_input_stream","text":"<pre><code>open_input_stream(path: str) -&gt; pa.PythonFile\n</code></pre> <p>Open an input stream for sequential reading.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The source to open for reading.</p> required <p>Returns:</p> Type Description <code>PythonFile</code> <p>NativeFile</p>"},{"location":"api/schema/#deltalake.fs.DeltaStorageHandler.open_output_stream","title":"open_output_stream","text":"<pre><code>open_output_stream(path: str, metadata: Optional[Dict[str, str]] = None) -&gt; pa.PythonFile\n</code></pre> <p>Open an output stream for sequential writing.</p> <p>If the target already exists, existing data is truncated.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The source to open for writing.</p> required <code>metadata</code> <code>Optional[Dict[str, str]]</code> <p>If not None, a mapping of string keys to string values.</p> <code>None</code> <p>Returns:</p> Type Description <code>PythonFile</code> <p>NativeFile</p>"},{"location":"api/storage/","title":"Storage","text":""},{"location":"api/storage/#delta-storage-handler","title":"Delta Storage Handler","text":""},{"location":"api/storage/#deltalake.fs.DeltaStorageHandler","title":"DeltaStorageHandler","text":"<pre><code>DeltaStorageHandler(root: str, options: dict[str, str] | None = None, known_sizes: dict[str, int] | None = None)\n</code></pre> <p>             Bases: <code>DeltaFileSystemHandler</code>, <code>FileSystemHandler</code></p> <p>DeltaStorageHandler is a concrete implementations of a PyArrow FileSystemHandler.</p>"},{"location":"api/storage/#deltalake.fs.DeltaStorageHandler.get_file_info_selector","title":"get_file_info_selector","text":"<pre><code>get_file_info_selector(selector: FileSelector) -&gt; List[FileInfo]\n</code></pre> <p>Get info for the files defined by FileSelector.</p> <p>Parameters:</p> Name Type Description Default <code>selector</code> <code>FileSelector</code> <p>FileSelector object</p> required <p>Returns:</p> Type Description <code>List[FileInfo]</code> <p>list of file info objects</p>"},{"location":"api/storage/#deltalake.fs.DeltaStorageHandler.open_input_file","title":"open_input_file","text":"<pre><code>open_input_file(path: str) -&gt; pa.PythonFile\n</code></pre> <p>Open an input file for random access reading.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The source to open for reading.</p> required <p>Returns:</p> Type Description <code>PythonFile</code> <p>NativeFile</p>"},{"location":"api/storage/#deltalake.fs.DeltaStorageHandler.open_input_stream","title":"open_input_stream","text":"<pre><code>open_input_stream(path: str) -&gt; pa.PythonFile\n</code></pre> <p>Open an input stream for sequential reading.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The source to open for reading.</p> required <p>Returns:</p> Type Description <code>PythonFile</code> <p>NativeFile</p>"},{"location":"api/storage/#deltalake.fs.DeltaStorageHandler.open_output_stream","title":"open_output_stream","text":"<pre><code>open_output_stream(path: str, metadata: Optional[Dict[str, str]] = None) -&gt; pa.PythonFile\n</code></pre> <p>Open an output stream for sequential writing.</p> <p>If the target already exists, existing data is truncated.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The source to open for writing.</p> required <code>metadata</code> <code>Optional[Dict[str, str]]</code> <p>If not None, a mapping of string keys to string values.</p> <code>None</code> <p>Returns:</p> Type Description <code>PythonFile</code> <p>NativeFile</p>"},{"location":"usage/","title":"Usage","text":"<p>A DeltaTable represents the state of a delta table at a particular version. This includes which files are currently part of the table, the schema of the table, and other metadata such as creation time.</p>  Python Rust <p> <code>DeltaTable</code> <pre><code>from deltalake import DeltaTable\n\ndt = DeltaTable(\"../rust/tests/data/delta-0.2.0\")\nprint(f\"Version: {dt.version()}\")\nprint(f\"Files: {dt.files()}\")\n</code></pre></p> <p> <code>DeltaTable</code> <pre><code>let table = deltalake::open_table(\"../rust/tests/data/simple_table\").await.unwrap();\nprintln!(\"Version: {}\", table.version());\nprintln!(\"Files: {}\", table.get_files());\n</code></pre></p>"},{"location":"usage/examining-table/","title":"Examining a Table","text":""},{"location":"usage/examining-table/#metadata","title":"Metadata","text":"<p>The delta log maintains basic metadata about a table, including:</p> <ul> <li>A unique <code>id</code></li> <li>A <code>name</code>, if provided</li> <li>A <code>description</code>, if provided</li> <li>The list of <code>partitionColumns</code>.</li> <li>The <code>created_time</code> of the table</li> <li>A map of table <code>configuration</code>. This includes fields such as     <code>delta.appendOnly</code>, which if <code>true</code> indicates the table is not meant     to have data deleted from it.</li> </ul> <p>Get metadata from a table with the DeltaTable.metadata() method:</p> <pre><code>&gt;&gt;&gt; from deltalake import DeltaTable\n&gt;&gt;&gt; dt = DeltaTable(\"../rust/tests/data/simple_table\")\n&gt;&gt;&gt; dt.metadata()\nMetadata(id: 5fba94ed-9794-4965-ba6e-6ee3c0d22af9, name: None, description: None, partitionColumns: [], created_time: 1587968585495, configuration={})\n</code></pre>"},{"location":"usage/examining-table/#schema","title":"Schema","text":"<p>The schema for the table is also saved in the transaction log. It can either be retrieved in the Delta Lake form as Schema or as a PyArrow schema. The first allows you to introspect any column-level metadata stored in the schema, while the latter represents the schema the table will be loaded into.</p> <p>Use DeltaTable.schema to retrieve the delta lake schema:</p> <pre><code>&gt;&gt;&gt; from deltalake import DeltaTable\n&gt;&gt;&gt; dt = DeltaTable(\"../rust/tests/data/simple_table\")\n&gt;&gt;&gt; dt.schema()\nSchema([Field(id, PrimitiveType(\"long\"), nullable=True)])\n</code></pre> <p>These schemas have a JSON representation that can be retrieved. To reconstruct from json, use DeltaTable.schema.to_json().</p> <pre><code>&gt;&gt;&gt; dt.schema().to_json()\n'{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]}'\n</code></pre> <p>Use DeltaTable.schema.to_pyarrow() to retrieve the PyArrow schema:</p> <pre><code>&gt;&gt;&gt; dt.schema().to_pyarrow()\nid: int64\n</code></pre>"},{"location":"usage/examining-table/#history","title":"History","text":"<p>Depending on what system wrote the table, the delta table may have provenance information describing what operations were performed on the table, when, and by whom. This information is retained for 30 days by default, unless otherwise specified by the table configuration <code>delta.logRetentionDuration</code>.</p> <p>Note</p> <p>This information is not written by all writers and different writers may use different schemas to encode the actions. For Spark\\'s format, see: https://docs.delta.io/latest/delta-utility.html#history-schema</p> <p>To view the available history, use <code>DeltaTable.history</code>:</p> <pre><code>&gt;&gt;&gt; from deltalake import DeltaTable\n&gt;&gt;&gt; dt = DeltaTable(\"../rust/tests/data/simple_table\")\n&gt;&gt;&gt; dt.history()\n[{'timestamp': 1587968626537, 'operation': 'DELETE', 'operationParameters': {'predicate': '[\"((`id` % CAST(2 AS BIGINT)) = CAST(0 AS BIGINT))\"]'}, 'readVersion': 3, 'isBlindAppend': False},\n {'timestamp': 1587968614187, 'operation': 'UPDATE', 'operationParameters': {'predicate': '((id#697L % cast(2 as bigint)) = cast(0 as bigint))'}, 'readVersion': 2, 'isBlindAppend': False},\n {'timestamp': 1587968604143, 'operation': 'WRITE', 'operationParameters': {'mode': 'Overwrite', 'partitionBy': '[]'}, 'readVersion': 1, 'isBlindAppend': False},\n {'timestamp': 1587968596254, 'operation': 'MERGE', 'operationParameters': {'predicate': '(oldData.`id` = newData.`id`)'}, 'readVersion': 0, 'isBlindAppend': False},\n {'timestamp': 1587968586154, 'operation': 'WRITE', 'operationParameters': {'mode': 'ErrorIfExists', 'partitionBy': '[]'}, 'isBlindAppend': True}]\n</code></pre>"},{"location":"usage/examining-table/#current-add-actions","title":"Current Add Actions","text":"<p>The active state for a delta table is determined by the Add actions, which provide the list of files that are part of the table and metadata about them, such as creation time, size, and statistics. You can get a data frame of the add actions data using <code>DeltaTable.get_add_actions</code>:</p> <pre><code>&gt;&gt;&gt; from deltalake import DeltaTable\n&gt;&gt;&gt; dt = DeltaTable(\"../rust/tests/data/delta-0.8.0\")\n&gt;&gt;&gt; dt.get_add_actions(flatten=True).to_pandas()\n                                                    path  size_bytes   modification_time  data_change  num_records  null_count.value  min.value  max.value\n0  part-00000-c9b90f86-73e6-46c8-93ba-ff6bfaf892a...         440 2021-03-06 15:16:07         True            2                 0          0          2\n1  part-00000-04ec9591-0b73-459e-8d18-ba5711d6cbe...         440 2021-03-06 15:16:16         True            2                 0          2          4\n</code></pre> <p>This works even with past versions of the table:</p> <pre><code>&gt;&gt;&gt; dt = DeltaTable(\"../rust/tests/data/delta-0.8.0\", version=0)\n&gt;&gt;&gt; dt.get_add_actions(flatten=True).to_pandas()\n                                                path  size_bytes   modification_time  data_change  num_records  null_count.value  min.value  max.value\n0  part-00000-c9b90f86-73e6-46c8-93ba-ff6bfaf892a...         440 2021-03-06 15:16:07         True            2                 0          0          2\n1  part-00001-911a94a2-43f6-4acb-8620-5e68c265498...         445 2021-03-06 15:16:07         True            3                 0          2          4\n</code></pre>"},{"location":"usage/loading-table/","title":"Loading a Delta Table","text":"<p>To load the current version, use the constructor:</p> <pre><code>&gt;&gt;&gt; dt = DeltaTable(\"../rust/tests/data/delta-0.2.0\")\n</code></pre> <p>Depending on your storage backend, you could use the <code>storage_options</code> parameter to provide some configuration. Configuration is defined for specific backends - s3 options, azure options, gcs options.</p> <pre><code>&gt;&gt;&gt; storage_options = {\"AWS_ACCESS_KEY_ID\": \"THE_AWS_ACCESS_KEY_ID\", \"AWS_SECRET_ACCESS_KEY\":\"THE_AWS_SECRET_ACCESS_KEY\"}\n&gt;&gt;&gt; dt = DeltaTable(\"../rust/tests/data/delta-0.2.0\", storage_options=storage_options)\n</code></pre> <p>The configuration can also be provided via the environment, and the basic service provider is derived from the URL being used. We try to support many of the well-known formats to identify basic service properties.</p> <p>S3:</p> <ul> <li>s3://\\&lt;bucket&gt;/\\&lt;path&gt;</li> <li>s3a://\\&lt;bucket&gt;/\\&lt;path&gt;</li> </ul> <p>Azure:</p> <ul> <li>az://\\&lt;container&gt;/\\&lt;path&gt;</li> <li>adl://\\&lt;container&gt;/\\&lt;path&gt;</li> <li>abfs://\\&lt;container&gt;/\\&lt;path&gt;</li> </ul> <p>GCS:</p> <ul> <li>gs://\\&lt;bucket&gt;/\\&lt;path&gt;</li> </ul> <p>Alternatively, if you have a data catalog you can load it by reference to a database and table name. Currently only AWS Glue is supported.</p> <p>For AWS Glue catalog, use AWS environment variables to authenticate.</p> <pre><code>&gt;&gt;&gt; from deltalake import DeltaTable\n&gt;&gt;&gt; from deltalake import DataCatalog\n&gt;&gt;&gt; database_name = \"simple_database\"\n&gt;&gt;&gt; table_name = \"simple_table\"\n&gt;&gt;&gt; data_catalog = DataCatalog.AWS\n&gt;&gt;&gt; dt = DeltaTable.from_data_catalog(data_catalog=data_catalog, database_name=database_name, table_name=table_name)\n&gt;&gt;&gt; dt.to_pyarrow_table().to_pydict()\n{'id': [5, 7, 9, 5, 6, 7, 8, 9]}\n</code></pre>"},{"location":"usage/loading-table/#custom-storage-backends","title":"Custom Storage Backends","text":"<p>While delta always needs its internal storage backend to work and be properly configured, in order to manage the delta log, it may sometime be advantageous - and is common practice in the arrow world - to customize the storage interface used for reading the bulk data.</p> <p><code>deltalake</code> will work with any storage compliant with <code>pyarrow.fs.FileSystem</code>, however the root of the filesystem has to be adjusted to point at the root of the Delta table. We can achieve this by wrapping the custom filesystem into a <code>pyarrow.fs.SubTreeFileSystem</code>.</p> <pre><code>import pyarrow.fs as fs\nfrom deltalake import DeltaTable\n\npath = \"&lt;path/to/table&gt;\"\nfilesystem = fs.SubTreeFileSystem(path, fs.LocalFileSystem())\n\ndt = DeltaTable(path)\nds = dt.to_pyarrow_dataset(filesystem=filesystem)\n</code></pre> <p>When using the pyarrow factory method for file systems, the normalized path is provided on creation. In case of S3 this would look something like:</p> <pre><code>import pyarrow.fs as fs\nfrom deltalake import DeltaTable\n\ntable_uri = \"s3://&lt;bucket&gt;/&lt;path&gt;\"\nraw_fs, normalized_path = fs.FileSystem.from_uri(table_uri)\nfilesystem = fs.SubTreeFileSystem(normalized_path, raw_fs)\n\ndt = DeltaTable(table_uri)\nds = dt.to_pyarrow_dataset(filesystem=filesystem)\n</code></pre>"},{"location":"usage/loading-table/#time-travel","title":"Time Travel","text":"<p>To load previous table states, you can provide the version number you wish to load:</p> <pre><code>&gt;&gt;&gt; dt = DeltaTable(\"../rust/tests/data/simple_table\", version=2)\n</code></pre> <p>Once you\\'ve loaded a table, you can also change versions using either a version number or datetime string:</p> <pre><code>&gt;&gt;&gt; dt.load_version(1)\n&gt;&gt;&gt; dt.load_with_datetime(\"2021-11-04 00:05:23.283+00:00\")\n</code></pre> <p>Warning</p> <p>Previous table versions may not exist if they have been vacuumed, in which case an exception will be thrown. See Vacuuming tables for more information.</p>"},{"location":"usage/managing-tables/","title":"Managing Delta Tables","text":""},{"location":"usage/managing-tables/#vacuuming-tables","title":"Vacuuming tables","text":"<p>Vacuuming a table will delete any files that have been marked for deletion. This may make some past versions of a table invalid, so this can break time travel. However, it will save storage space. Vacuum will retain files in a certain window, by default one week, so time travel will still work in shorter ranges.</p> <p>Delta tables usually don't delete old files automatically, so vacuuming regularly is considered good practice, unless the table is only appended to.</p> <p>Use <code>DeltaTable.vacuum</code> to perform the vacuum operation. Note that to prevent accidental deletion, the function performs a dry-run by default: it will only list the files to be deleted. Pass <code>dry_run=False</code> to actually delete files.</p> <pre><code>&gt;&gt;&gt; dt = DeltaTable(\"../rust/tests/data/simple_table\")\n&gt;&gt;&gt; dt.vacuum()\n['../rust/tests/data/simple_table/part-00006-46f2ff20-eb5d-4dda-8498-7bfb2940713b-c000.snappy.parquet',\n '../rust/tests/data/simple_table/part-00190-8ac0ae67-fb1d-461d-a3d3-8dc112766ff5-c000.snappy.parquet',\n '../rust/tests/data/simple_table/part-00164-bf40481c-4afd-4c02-befa-90f056c2d77a-c000.snappy.parquet',\n ...]\n&gt;&gt;&gt; dt.vacuum(dry_run=False) # Don't run this unless you are sure!\n</code></pre>"},{"location":"usage/managing-tables/#optimizing-tables","title":"Optimizing tables","text":"<p>Optimizing tables is not currently supported.</p>"},{"location":"usage/querying-delta-tables/","title":"Querying Delta Tables","text":"<p>Delta tables can be queried in several ways. By loading as Arrow data or an Arrow dataset, they can be used by compatible engines such as Pandas and DuckDB. By passing on the list of files, they can be loaded into other engines such as Dask.</p> <p>Delta tables are often larger than can fit into memory on a single computer, so this module provides ways to read only the parts of the data you need. Partition filters allow you to skip reading files that are part of irrelevant partitions. Only loading the columns required also saves memory. Finally, some methods allow reading tables batch-by-batch, allowing you to process the whole table while only having a portion loaded at any given time.</p> <p>To load into Pandas or a PyArrow table use the <code>DeltaTable.to_pandas</code> and <code>DeltaTable.to_pyarrow_table</code> methods, respectively. Both of these support filtering partitions and selecting particular columns.</p> <pre><code>&gt;&gt;&gt; from deltalake import DeltaTable\n&gt;&gt;&gt; dt = DeltaTable(\"../rust/tests/data/delta-0.8.0-partitioned\")\n&gt;&gt;&gt; dt.schema().to_pyarrow()\nvalue: string\nyear: string\nmonth: string\nday: string\n&gt;&gt;&gt; dt.to_pandas(partitions=[(\"year\", \"=\", \"2021\")], columns=[\"value\"])\n      value\n0     6\n1     7\n2     5\n3     4\n&gt;&gt;&gt; dt.to_pyarrow_table(partitions=[(\"year\", \"=\", \"2021\")], columns=[\"value\"])\npyarrow.Table\nvalue: string\n</code></pre> <p>Converting to a PyArrow Dataset allows you to filter on columns other than partition columns and load the result as a stream of batches rather than a single table. Convert to a dataset using <code>DeltaTable.to_pyarrow_dataset</code>. Filters applied to datasets will use the partition values and file statistics from the Delta transaction log and push down any other filters to the scanning operation.</p> <pre><code>&gt;&gt;&gt; import pyarrow.dataset as ds\n&gt;&gt;&gt; dataset = dt.to_pyarrow_dataset()\n&gt;&gt;&gt; condition = (ds.field(\"year\") == \"2021\") &amp; (ds.field(\"value\") &gt; \"4\")\n&gt;&gt;&gt; dataset.to_table(filter=condition, columns=[\"value\"]).to_pandas()\n  value\n0     6\n1     7\n2     5\n&gt;&gt;&gt; batch_iter = dataset.to_batches(filter=condition, columns=[\"value\"], batch_size=2)\n&gt;&gt;&gt; for batch in batch_iter: print(batch.to_pandas())\n  value\n0     6\n1     7\n  value\n0     5\n</code></pre> <p>PyArrow datasets may also be passed to compatible query engines, such as DuckDB</p> <pre><code>&gt;&gt;&gt; import duckdb\n&gt;&gt;&gt; ex_data = duckdb.arrow(dataset)\n&gt;&gt;&gt; ex_data.filter(\"year = 2021 and value &gt; 4\").project(\"value\")\n---------------------\n-- Expression Tree --\n---------------------\nProjection [value]\n  Filter [year=2021 AND value&gt;4]\n    arrow_scan(140409099470144, 4828104688, 1000000)\n\n---------------------\n-- Result Columns  --\n---------------------\n- value (VARCHAR)\n\n---------------------\n-- Result Preview  --\n---------------------\nvalue\nVARCHAR\n[ Rows: 3]\n6\n7\n5\n</code></pre> <p>Finally, you can always pass the list of file paths to an engine. For example, you can pass them to <code>dask.dataframe.read_parquet</code>:</p> <pre><code>&gt;&gt;&gt; import dask.dataframe as dd\n&gt;&gt;&gt; df = dd.read_parquet(dt.file_uris())\n&gt;&gt;&gt; df\nDask DataFrame Structure:\n                value             year            month              day\nnpartitions=6\n               object  category[known]  category[known]  category[known]\n                  ...              ...              ...              ...\n...               ...              ...              ...              ...\n                  ...              ...              ...              ...\n                  ...              ...              ...              ...\nDask Name: read-parquet, 6 tasks\n&gt;&gt;&gt; df.compute()\n  value  year month day\n0     1  2020     1   1\n0     2  2020     2   3\n0     3  2020     2   5\n0     4  2021     4   5\n0     5  2021    12   4\n0     6  2021    12  20\n1     7  2021    12  20\n</code></pre>"},{"location":"usage/writing-delta-tables/","title":"Writing Delta Tables","text":"<p>For overwrites and appends, use <code>write_deltalake</code>. If the table does not already exist, it will be created. The <code>data</code> parameter will accept a Pandas DataFrame, a PyArrow Table, or an iterator of PyArrow Record Batches.</p> <pre><code>&gt;&gt;&gt; from deltalake.writer import write_deltalake\n&gt;&gt;&gt; df = pd.DataFrame({'x': [1, 2, 3]})\n&gt;&gt;&gt; write_deltalake('path/to/table', df)\n</code></pre> <p>Note: <code>write_deltalake</code> accepts a Pandas DataFrame, but will convert it to a Arrow table before writing. See caveats in <code>pyarrow:python/pandas</code>.</p> <p>By default, writes create a new table and error if it already exists. This is controlled by the <code>mode</code> parameter, which mirrors the behavior of Spark's <code>pyspark.sql.DataFrameWriter.saveAsTable</code> DataFrame method. To overwrite pass in <code>mode='overwrite'</code> and to append pass in <code>mode='append'</code>:</p> <pre><code>&gt;&gt;&gt; write_deltalake('path/to/table', df, mode='overwrite')\n&gt;&gt;&gt; write_deltalake('path/to/table', df, mode='append')\n</code></pre> <p><code>write_deltalake</code> will raise <code>ValueError</code> if the schema of the data passed to it differs from the existing table's schema. If you wish to alter the schema as part of an overwrite pass in <code>overwrite_schema=True</code>.</p>"},{"location":"usage/writing-delta-tables/#overwriting-a-partition","title":"Overwriting a partition","text":"<p>You can overwrite a specific partition by using <code>mode=\"overwrite\"</code> together with <code>partition_filters</code>. This will remove all files within the matching partition and insert your data as new files. This can only be done on one partition at a time. All of the input data must belong to that partition or else the method will raise an error.</p> <pre><code>&gt;&gt;&gt; from deltalake.writer import write_deltalake\n&gt;&gt;&gt; df = pd.DataFrame({'x': [1, 2, 3], 'y': ['a', 'a', 'b']})\n&gt;&gt;&gt; write_deltalake('path/to/table', df, partition_by=['y'])\n\n&gt;&gt;&gt; table = DeltaTable('path/to/table')\n&gt;&gt;&gt; df2 = pd.DataFrame({'x': [100], 'y': ['b']})\n&gt;&gt;&gt; write_deltalake(table, df2, partition_filters=[('y', '=', 'b')], mode=\"overwrite\")\n\n&gt;&gt;&gt; table.to_pandas()\n     x  y\n0    1  a\n1    2  a\n2  100  b\n</code></pre> <p>This method could also be used to insert a new partition if one doesn't already exist, making this operation idempotent.</p>"}]}